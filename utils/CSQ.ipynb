{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deephashh/lib/python3.8/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/deephashh/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "          1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "          1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "          1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "          1., -1.,  1., -1.,  1., -1.,  1., -1.],\n",
       "        [ 1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "         -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
       "          1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "         -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
       "          1.,  1., -1., -1.,  1.,  1., -1., -1.],\n",
       "        [ 1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
       "         -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "          1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
       "         -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "          1., -1., -1.,  1.,  1., -1., -1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
       "         -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,\n",
       "         -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
       "          1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
       "          1.,  1.,  1.,  1., -1., -1., -1., -1.],\n",
       "        [ 1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "         -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "         -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
       "          1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
       "          1., -1.,  1., -1., -1.,  1., -1.,  1.],\n",
       "        [ 1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
       "          1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
       "         -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,\n",
       "         -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
       "          1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
       "        [ 1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
       "          1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "         -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "         -1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
       "          1., -1., -1.,  1., -1.,  1.,  1., -1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [ 1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
       "         -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
       "         -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "         -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "         -1.,  1., -1.,  1., -1.,  1., -1.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import hash_model as image_hash_model\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "top_k = 1000\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "lr = 0.001 #0.05\n",
    "weight_decay = 10 ** -5\n",
    "lambda1 = 0.01\n",
    "hash_bits = 64 # 假设哈希编码的位数\n",
    "model_name = \"vgg11\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 1. 加载CIFAR-10数据集\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model = image_hash_model.HASH_Net(model_name, hash_bits)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.3, last_epoch=-1)\n",
    "\n",
    "# 3. 加载哈希编码\n",
    "with open('../labels/64_cifar10_10_class.pkl', 'rb') as f:\n",
    "    label_hash_codes= torch.load(f)\n",
    "label_hash_codes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test Accuracy: 84.36%\n",
      "Model saved with accuracy: 84.36%\n",
      "epoch: 0, lr: 0.00100 iter_num: 0, time: 17.771, loss: 0.275\n",
      "epoch: 0, lr: 0.00100 iter_num: 100, time: 19.755, loss: 0.396\n",
      "epoch: 0, lr: 0.00100 iter_num: 200, time: 21.785, loss: 0.457\n",
      "epoch: 0, lr: 0.00100 iter_num: 300, time: 23.943, loss: 0.334\n",
      "epoch: 0, lr: 0.00100 iter_num: 400, time: 26.153, loss: 0.251\n",
      "epoch: 0, lr: 0.00100 iter_num: 500, time: 28.401, loss: 0.370\n",
      "epoch: 0, lr: 0.00100 iter_num: 600, time: 30.273, loss: 0.382\n",
      "epoch: 0, lr: 0.00100 iter_num: 700, time: 32.363, loss: 0.249\n",
      "epoch: 0, lr: 0.00100 iter_num: 800, time: 34.335, loss: 0.301\n",
      "epoch: 0, lr: 0.00100 iter_num: 900, time: 36.405, loss: 0.522\n",
      "epoch: 0, lr: 0.00100 iter_num: 1000, time: 38.368, loss: 0.356\n",
      "epoch: 0, lr: 0.00100 iter_num: 1100, time: 40.413, loss: 0.320\n",
      "epoch: 0, lr: 0.00100 iter_num: 1200, time: 42.718, loss: 0.360\n",
      "epoch: 0, lr: 0.00100 iter_num: 1300, time: 44.959, loss: 0.304\n",
      "epoch: 0, lr: 0.00100 iter_num: 1400, time: 47.096, loss: 0.151\n",
      "epoch: 0, lr: 0.00100 iter_num: 1500, time: 49.221, loss: 0.316\n",
      "epoch: 0, lr: 0.00100 iter_num: 1600, time: 51.323, loss: 0.296\n",
      "epoch: 0, lr: 0.00100 iter_num: 1700, time: 53.609, loss: 0.377\n",
      "epoch: 0, lr: 0.00100 iter_num: 1800, time: 55.753, loss: 0.220\n",
      "epoch: 0, lr: 0.00100 iter_num: 1900, time: 58.024, loss: 0.427\n",
      "epoch: 0, lr: 0.00100 iter_num: 2000, time: 60.254, loss: 0.261\n",
      "epoch: 0, lr: 0.00100 iter_num: 2100, time: 62.594, loss: 0.301\n",
      "epoch: 0, lr: 0.00100 iter_num: 2200, time: 64.918, loss: 0.246\n",
      "epoch: 0, lr: 0.00100 iter_num: 2300, time: 67.181, loss: 0.317\n",
      "epoch: 0, lr: 0.00100 iter_num: 2400, time: 69.232, loss: 0.288\n",
      "epoch: 0, lr: 0.00100 iter_num: 2500, time: 71.291, loss: 0.320\n",
      "epoch: 0, lr: 0.00100 iter_num: 2600, time: 73.243, loss: 0.362\n",
      "epoch: 0, lr: 0.00100 iter_num: 2700, time: 75.506, loss: 0.237\n",
      "epoch: 0, lr: 0.00100 iter_num: 2800, time: 77.727, loss: 0.336\n",
      "epoch: 0, lr: 0.00100 iter_num: 2900, time: 79.909, loss: 0.303\n",
      "epoch: 0, lr: 0.00100 iter_num: 3000, time: 82.269, loss: 0.287\n",
      "epoch: 0, lr: 0.00100 iter_num: 3100, time: 84.654, loss: 0.455\n",
      "epoch: 0, lr: 0.00100 iter_num: 3200, time: 86.823, loss: 0.241\n",
      "epoch: 0, lr: 0.00100 iter_num: 3300, time: 89.038, loss: 0.313\n",
      "epoch: 0, lr: 0.00100 iter_num: 3400, time: 91.230, loss: 0.273\n",
      "epoch: 0, lr: 0.00100 iter_num: 3500, time: 93.524, loss: 0.597\n",
      "epoch: 0, lr: 0.00100 iter_num: 3600, time: 95.776, loss: 0.318\n",
      "epoch: 0, lr: 0.00100 iter_num: 3700, time: 97.998, loss: 0.226\n",
      "epoch: 0, lr: 0.00100 iter_num: 3800, time: 100.108, loss: 0.397\n",
      "epoch: 0, lr: 0.00100 iter_num: 3900, time: 102.438, loss: 0.233\n",
      "epoch: 0, lr: 0.00100 iter_num: 4000, time: 104.616, loss: 0.299\n",
      "epoch: 0, lr: 0.00100 iter_num: 4100, time: 106.864, loss: 0.255\n",
      "epoch: 0, lr: 0.00100 iter_num: 4200, time: 108.959, loss: 0.463\n",
      "epoch: 0, lr: 0.00100 iter_num: 4300, time: 111.133, loss: 0.295\n",
      "epoch: 0, lr: 0.00100 iter_num: 4400, time: 113.435, loss: 0.322\n",
      "epoch: 0, lr: 0.00100 iter_num: 4500, time: 115.655, loss: 0.434\n",
      "epoch: 0, lr: 0.00100 iter_num: 4600, time: 117.873, loss: 0.336\n",
      "epoch: 0, lr: 0.00100 iter_num: 4700, time: 120.109, loss: 0.437\n",
      "epoch: 0, lr: 0.00100 iter_num: 4800, time: 122.423, loss: 0.257\n",
      "epoch: 0, lr: 0.00100 iter_num: 4900, time: 124.623, loss: 0.482\n",
      "epoch: 1, lr: 0.00100 iter_num: 0, time: 126.841, loss: 0.279\n",
      "epoch: 1, lr: 0.00100 iter_num: 100, time: 128.981, loss: 0.101\n",
      "epoch: 1, lr: 0.00100 iter_num: 200, time: 131.253, loss: 0.393\n",
      "epoch: 1, lr: 0.00100 iter_num: 300, time: 133.486, loss: 0.206\n",
      "epoch: 1, lr: 0.00100 iter_num: 400, time: 135.751, loss: 0.274\n",
      "epoch: 1, lr: 0.00100 iter_num: 500, time: 138.090, loss: 0.229\n",
      "epoch: 1, lr: 0.00100 iter_num: 600, time: 140.436, loss: 0.396\n",
      "epoch: 1, lr: 0.00100 iter_num: 700, time: 142.782, loss: 0.232\n",
      "epoch: 1, lr: 0.00100 iter_num: 800, time: 145.024, loss: 0.203\n",
      "epoch: 1, lr: 0.00100 iter_num: 900, time: 147.315, loss: 0.413\n",
      "epoch: 1, lr: 0.00100 iter_num: 1000, time: 149.734, loss: 0.239\n",
      "epoch: 1, lr: 0.00100 iter_num: 1100, time: 151.977, loss: 0.233\n",
      "epoch: 1, lr: 0.00100 iter_num: 1200, time: 154.319, loss: 0.326\n",
      "epoch: 1, lr: 0.00100 iter_num: 1300, time: 156.636, loss: 0.227\n",
      "epoch: 1, lr: 0.00100 iter_num: 1400, time: 158.943, loss: 0.307\n",
      "epoch: 1, lr: 0.00100 iter_num: 1500, time: 161.375, loss: 0.223\n",
      "epoch: 1, lr: 0.00100 iter_num: 1600, time: 163.646, loss: 0.151\n",
      "epoch: 1, lr: 0.00100 iter_num: 1700, time: 165.961, loss: 0.383\n",
      "epoch: 1, lr: 0.00100 iter_num: 1800, time: 168.198, loss: 0.362\n",
      "epoch: 1, lr: 0.00100 iter_num: 1900, time: 170.470, loss: 0.288\n",
      "epoch: 1, lr: 0.00100 iter_num: 2000, time: 172.861, loss: 0.292\n",
      "epoch: 1, lr: 0.00100 iter_num: 2100, time: 175.149, loss: 0.276\n",
      "epoch: 1, lr: 0.00100 iter_num: 2200, time: 177.551, loss: 0.273\n",
      "epoch: 1, lr: 0.00100 iter_num: 2300, time: 179.913, loss: 0.231\n",
      "epoch: 1, lr: 0.00100 iter_num: 2400, time: 182.174, loss: 0.212\n",
      "epoch: 1, lr: 0.00100 iter_num: 2500, time: 184.471, loss: 0.412\n",
      "epoch: 1, lr: 0.00100 iter_num: 2600, time: 186.818, loss: 0.264\n",
      "epoch: 1, lr: 0.00100 iter_num: 2700, time: 189.137, loss: 0.253\n",
      "epoch: 1, lr: 0.00100 iter_num: 2800, time: 191.554, loss: 0.248\n",
      "epoch: 1, lr: 0.00100 iter_num: 2900, time: 193.852, loss: 0.324\n",
      "epoch: 1, lr: 0.00100 iter_num: 3000, time: 196.142, loss: 0.257\n",
      "epoch: 1, lr: 0.00100 iter_num: 3100, time: 198.520, loss: 0.221\n",
      "epoch: 1, lr: 0.00100 iter_num: 3200, time: 200.829, loss: 0.250\n",
      "epoch: 1, lr: 0.00100 iter_num: 3300, time: 203.151, loss: 0.272\n",
      "epoch: 1, lr: 0.00100 iter_num: 3400, time: 205.413, loss: 0.196\n",
      "epoch: 1, lr: 0.00100 iter_num: 3500, time: 207.865, loss: 0.393\n",
      "epoch: 1, lr: 0.00100 iter_num: 3600, time: 210.199, loss: 0.286\n",
      "epoch: 1, lr: 0.00100 iter_num: 3700, time: 212.459, loss: 0.235\n",
      "epoch: 1, lr: 0.00100 iter_num: 3800, time: 214.824, loss: 0.106\n",
      "epoch: 1, lr: 0.00100 iter_num: 3900, time: 217.274, loss: 0.346\n",
      "epoch: 1, lr: 0.00100 iter_num: 4000, time: 219.522, loss: 0.322\n",
      "epoch: 1, lr: 0.00100 iter_num: 4100, time: 221.881, loss: 0.323\n",
      "epoch: 1, lr: 0.00100 iter_num: 4200, time: 224.182, loss: 0.289\n",
      "epoch: 1, lr: 0.00100 iter_num: 4300, time: 226.611, loss: 0.211\n",
      "epoch: 1, lr: 0.00100 iter_num: 4400, time: 229.021, loss: 0.334\n",
      "epoch: 1, lr: 0.00100 iter_num: 4500, time: 231.359, loss: 0.265\n",
      "epoch: 1, lr: 0.00100 iter_num: 4600, time: 233.691, loss: 0.276\n",
      "epoch: 1, lr: 0.00100 iter_num: 4700, time: 236.087, loss: 0.163\n",
      "epoch: 1, lr: 0.00100 iter_num: 4800, time: 238.429, loss: 0.310\n",
      "epoch: 1, lr: 0.00100 iter_num: 4900, time: 240.695, loss: 0.222\n",
      "epoch: 2, lr: 0.00100 iter_num: 0, time: 243.006, loss: 0.212\n",
      "epoch: 2, lr: 0.00100 iter_num: 100, time: 245.326, loss: 0.295\n",
      "epoch: 2, lr: 0.00100 iter_num: 200, time: 247.583, loss: 0.276\n",
      "epoch: 2, lr: 0.00100 iter_num: 300, time: 249.967, loss: 0.281\n",
      "epoch: 2, lr: 0.00100 iter_num: 400, time: 252.294, loss: 0.381\n",
      "epoch: 2, lr: 0.00100 iter_num: 500, time: 254.744, loss: 0.240\n",
      "epoch: 2, lr: 0.00100 iter_num: 600, time: 257.155, loss: 0.284\n",
      "epoch: 2, lr: 0.00100 iter_num: 700, time: 259.407, loss: 0.281\n",
      "epoch: 2, lr: 0.00100 iter_num: 800, time: 261.697, loss: 0.278\n",
      "epoch: 2, lr: 0.00100 iter_num: 900, time: 264.107, loss: 0.387\n",
      "epoch: 2, lr: 0.00100 iter_num: 1000, time: 266.409, loss: 0.245\n",
      "epoch: 2, lr: 0.00100 iter_num: 1100, time: 268.805, loss: 0.343\n",
      "epoch: 2, lr: 0.00100 iter_num: 1200, time: 271.159, loss: 0.330\n",
      "epoch: 2, lr: 0.00100 iter_num: 1300, time: 273.568, loss: 0.174\n",
      "epoch: 2, lr: 0.00100 iter_num: 1400, time: 275.952, loss: 0.285\n",
      "epoch: 2, lr: 0.00100 iter_num: 1500, time: 278.278, loss: 0.303\n",
      "epoch: 2, lr: 0.00100 iter_num: 1600, time: 280.544, loss: 0.277\n",
      "epoch: 2, lr: 0.00100 iter_num: 1700, time: 282.842, loss: 0.262\n",
      "epoch: 2, lr: 0.00100 iter_num: 1800, time: 285.288, loss: 0.251\n",
      "epoch: 2, lr: 0.00100 iter_num: 1900, time: 287.699, loss: 0.209\n",
      "epoch: 2, lr: 0.00100 iter_num: 2000, time: 290.048, loss: 0.236\n",
      "epoch: 2, lr: 0.00100 iter_num: 2100, time: 292.369, loss: 0.288\n",
      "epoch: 2, lr: 0.00100 iter_num: 2200, time: 294.926, loss: 0.240\n",
      "epoch: 2, lr: 0.00100 iter_num: 2300, time: 297.267, loss: 0.228\n",
      "epoch: 2, lr: 0.00100 iter_num: 2400, time: 299.515, loss: 0.304\n",
      "epoch: 2, lr: 0.00100 iter_num: 2500, time: 301.756, loss: 0.303\n",
      "epoch: 2, lr: 0.00100 iter_num: 2600, time: 304.200, loss: 0.385\n",
      "epoch: 2, lr: 0.00100 iter_num: 2700, time: 306.495, loss: 0.288\n",
      "epoch: 2, lr: 0.00100 iter_num: 2800, time: 308.870, loss: 0.158\n",
      "epoch: 2, lr: 0.00100 iter_num: 2900, time: 311.170, loss: 0.200\n",
      "epoch: 2, lr: 0.00100 iter_num: 3000, time: 313.746, loss: 0.251\n",
      "epoch: 2, lr: 0.00100 iter_num: 3100, time: 316.054, loss: 0.283\n",
      "epoch: 2, lr: 0.00100 iter_num: 3200, time: 318.355, loss: 0.298\n",
      "epoch: 2, lr: 0.00100 iter_num: 3300, time: 320.502, loss: 0.229\n",
      "epoch: 2, lr: 0.00100 iter_num: 3400, time: 322.917, loss: 0.240\n",
      "epoch: 2, lr: 0.00100 iter_num: 3500, time: 325.227, loss: 0.245\n",
      "epoch: 2, lr: 0.00100 iter_num: 3600, time: 327.635, loss: 0.246\n",
      "epoch: 2, lr: 0.00100 iter_num: 3700, time: 329.982, loss: 0.177\n",
      "epoch: 2, lr: 0.00100 iter_num: 3800, time: 332.358, loss: 0.210\n",
      "epoch: 2, lr: 0.00100 iter_num: 3900, time: 334.863, loss: 0.282\n",
      "epoch: 2, lr: 0.00100 iter_num: 4000, time: 337.163, loss: 0.293\n",
      "epoch: 2, lr: 0.00100 iter_num: 4100, time: 339.429, loss: 0.204\n",
      "epoch: 2, lr: 0.00100 iter_num: 4200, time: 341.832, loss: 0.241\n",
      "epoch: 2, lr: 0.00100 iter_num: 4300, time: 344.077, loss: 0.231\n",
      "epoch: 2, lr: 0.00100 iter_num: 4400, time: 346.461, loss: 0.332\n",
      "epoch: 2, lr: 0.00100 iter_num: 4500, time: 348.772, loss: 0.225\n",
      "epoch: 2, lr: 0.00100 iter_num: 4600, time: 351.019, loss: 0.268\n",
      "epoch: 2, lr: 0.00100 iter_num: 4700, time: 353.609, loss: 0.356\n",
      "epoch: 2, lr: 0.00100 iter_num: 4800, time: 355.887, loss: 0.276\n",
      "epoch: 2, lr: 0.00100 iter_num: 4900, time: 358.163, loss: 0.224\n",
      "epoch: 3, lr: 0.00100 iter_num: 0, time: 360.442, loss: 0.235\n",
      "epoch: 3, lr: 0.00100 iter_num: 100, time: 362.961, loss: 0.240\n",
      "epoch: 3, lr: 0.00100 iter_num: 200, time: 365.391, loss: 0.220\n",
      "epoch: 3, lr: 0.00100 iter_num: 300, time: 367.730, loss: 0.267\n",
      "epoch: 3, lr: 0.00100 iter_num: 400, time: 369.988, loss: 0.298\n",
      "epoch: 3, lr: 0.00100 iter_num: 500, time: 372.487, loss: 0.216\n",
      "epoch: 3, lr: 0.00100 iter_num: 600, time: 374.913, loss: 0.221\n",
      "epoch: 3, lr: 0.00100 iter_num: 700, time: 377.231, loss: 0.339\n",
      "epoch: 3, lr: 0.00100 iter_num: 800, time: 379.555, loss: 0.273\n",
      "epoch: 3, lr: 0.00100 iter_num: 900, time: 382.026, loss: 0.207\n",
      "epoch: 3, lr: 0.00100 iter_num: 1000, time: 384.216, loss: 0.266\n",
      "epoch: 3, lr: 0.00100 iter_num: 1100, time: 386.591, loss: 0.258\n",
      "epoch: 3, lr: 0.00100 iter_num: 1200, time: 388.895, loss: 0.196\n",
      "epoch: 3, lr: 0.00100 iter_num: 1300, time: 391.151, loss: 0.309\n",
      "epoch: 3, lr: 0.00100 iter_num: 1400, time: 393.727, loss: 0.344\n",
      "epoch: 3, lr: 0.00100 iter_num: 1500, time: 395.891, loss: 0.233\n",
      "epoch: 3, lr: 0.00100 iter_num: 1600, time: 398.183, loss: 0.196\n",
      "epoch: 3, lr: 0.00100 iter_num: 1700, time: 400.541, loss: 0.263\n",
      "epoch: 3, lr: 0.00100 iter_num: 1800, time: 402.879, loss: 0.229\n",
      "epoch: 3, lr: 0.00100 iter_num: 1900, time: 405.263, loss: 0.143\n",
      "epoch: 3, lr: 0.00100 iter_num: 2000, time: 407.311, loss: 0.294\n",
      "epoch: 3, lr: 0.00100 iter_num: 2100, time: 409.644, loss: 0.302\n",
      "epoch: 3, lr: 0.00100 iter_num: 2200, time: 412.158, loss: 0.200\n",
      "epoch: 3, lr: 0.00100 iter_num: 2300, time: 414.577, loss: 0.288\n",
      "epoch: 3, lr: 0.00100 iter_num: 2400, time: 416.858, loss: 0.379\n",
      "epoch: 3, lr: 0.00100 iter_num: 2500, time: 419.163, loss: 0.261\n",
      "epoch: 3, lr: 0.00100 iter_num: 2600, time: 421.612, loss: 0.222\n",
      "epoch: 3, lr: 0.00100 iter_num: 2700, time: 423.971, loss: 0.322\n",
      "epoch: 3, lr: 0.00100 iter_num: 2800, time: 426.315, loss: 0.220\n",
      "epoch: 3, lr: 0.00100 iter_num: 2900, time: 428.616, loss: 0.279\n",
      "epoch: 3, lr: 0.00100 iter_num: 3000, time: 431.029, loss: 0.158\n",
      "epoch: 3, lr: 0.00100 iter_num: 3100, time: 433.345, loss: 0.170\n",
      "epoch: 3, lr: 0.00100 iter_num: 3200, time: 435.623, loss: 0.252\n",
      "epoch: 3, lr: 0.00100 iter_num: 3300, time: 437.904, loss: 0.186\n",
      "epoch: 3, lr: 0.00100 iter_num: 3400, time: 440.246, loss: 0.255\n",
      "epoch: 3, lr: 0.00100 iter_num: 3500, time: 442.633, loss: 0.296\n",
      "epoch: 3, lr: 0.00100 iter_num: 3600, time: 444.930, loss: 0.333\n",
      "epoch: 3, lr: 0.00100 iter_num: 3700, time: 447.183, loss: 0.382\n",
      "epoch: 3, lr: 0.00100 iter_num: 3800, time: 449.451, loss: 0.237\n",
      "epoch: 3, lr: 0.00100 iter_num: 3900, time: 451.975, loss: 0.308\n",
      "epoch: 3, lr: 0.00100 iter_num: 4000, time: 454.279, loss: 0.305\n",
      "epoch: 3, lr: 0.00100 iter_num: 4100, time: 456.535, loss: 0.318\n",
      "epoch: 3, lr: 0.00100 iter_num: 4200, time: 458.962, loss: 0.287\n",
      "epoch: 3, lr: 0.00100 iter_num: 4300, time: 461.271, loss: 0.238\n",
      "epoch: 3, lr: 0.00100 iter_num: 4400, time: 463.635, loss: 0.279\n",
      "epoch: 3, lr: 0.00100 iter_num: 4500, time: 465.891, loss: 0.197\n",
      "epoch: 3, lr: 0.00100 iter_num: 4600, time: 468.143, loss: 0.182\n",
      "epoch: 3, lr: 0.00100 iter_num: 4700, time: 470.575, loss: 0.178\n",
      "epoch: 3, lr: 0.00100 iter_num: 4800, time: 472.959, loss: 0.270\n",
      "epoch: 3, lr: 0.00100 iter_num: 4900, time: 475.267, loss: 0.197\n",
      "epoch: 4, lr: 0.00100 iter_num: 0, time: 477.618, loss: 0.254\n",
      "epoch: 4, lr: 0.00100 iter_num: 100, time: 479.968, loss: 0.203\n",
      "epoch: 4, lr: 0.00100 iter_num: 200, time: 482.354, loss: 0.244\n",
      "epoch: 4, lr: 0.00100 iter_num: 300, time: 484.556, loss: 0.244\n",
      "epoch: 4, lr: 0.00100 iter_num: 400, time: 486.868, loss: 0.255\n",
      "epoch: 4, lr: 0.00100 iter_num: 500, time: 489.262, loss: 0.294\n",
      "epoch: 4, lr: 0.00100 iter_num: 600, time: 491.638, loss: 0.147\n",
      "epoch: 4, lr: 0.00100 iter_num: 700, time: 493.926, loss: 0.190\n",
      "epoch: 4, lr: 0.00100 iter_num: 800, time: 496.240, loss: 0.305\n",
      "epoch: 4, lr: 0.00100 iter_num: 900, time: 498.630, loss: 0.299\n",
      "epoch: 4, lr: 0.00100 iter_num: 1000, time: 501.075, loss: 0.249\n",
      "epoch: 4, lr: 0.00100 iter_num: 1100, time: 503.371, loss: 0.160\n",
      "epoch: 4, lr: 0.00100 iter_num: 1200, time: 505.639, loss: 0.252\n",
      "epoch: 4, lr: 0.00100 iter_num: 1300, time: 508.170, loss: 0.224\n",
      "epoch: 4, lr: 0.00100 iter_num: 1400, time: 510.667, loss: 0.207\n",
      "epoch: 4, lr: 0.00100 iter_num: 1500, time: 513.028, loss: 0.222\n",
      "epoch: 4, lr: 0.00100 iter_num: 1600, time: 515.239, loss: 0.274\n",
      "epoch: 4, lr: 0.00100 iter_num: 1700, time: 517.700, loss: 0.302\n",
      "epoch: 4, lr: 0.00100 iter_num: 1800, time: 519.985, loss: 0.171\n",
      "epoch: 4, lr: 0.00100 iter_num: 1900, time: 522.380, loss: 0.216\n",
      "epoch: 4, lr: 0.00100 iter_num: 2000, time: 524.667, loss: 0.185\n",
      "epoch: 4, lr: 0.00100 iter_num: 2100, time: 526.848, loss: 0.239\n",
      "epoch: 4, lr: 0.00100 iter_num: 2200, time: 529.222, loss: 0.178\n",
      "epoch: 4, lr: 0.00100 iter_num: 2300, time: 531.533, loss: 0.198\n",
      "epoch: 4, lr: 0.00100 iter_num: 2400, time: 533.801, loss: 0.368\n",
      "epoch: 4, lr: 0.00100 iter_num: 2500, time: 536.154, loss: 0.216\n",
      "epoch: 4, lr: 0.00100 iter_num: 2600, time: 538.550, loss: 0.162\n",
      "epoch: 4, lr: 0.00100 iter_num: 2700, time: 540.954, loss: 0.238\n",
      "epoch: 4, lr: 0.00100 iter_num: 2800, time: 543.205, loss: 0.195\n",
      "epoch: 4, lr: 0.00100 iter_num: 2900, time: 545.485, loss: 0.204\n",
      "epoch: 4, lr: 0.00100 iter_num: 3000, time: 547.891, loss: 0.245\n",
      "epoch: 4, lr: 0.00100 iter_num: 3100, time: 550.239, loss: 0.220\n",
      "epoch: 4, lr: 0.00100 iter_num: 3200, time: 552.523, loss: 0.213\n",
      "epoch: 4, lr: 0.00100 iter_num: 3300, time: 554.810, loss: 0.238\n",
      "epoch: 4, lr: 0.00100 iter_num: 3400, time: 557.236, loss: 0.211\n",
      "epoch: 4, lr: 0.00100 iter_num: 3500, time: 559.566, loss: 0.156\n",
      "epoch: 4, lr: 0.00100 iter_num: 3600, time: 561.863, loss: 0.310\n",
      "epoch: 4, lr: 0.00100 iter_num: 3700, time: 564.172, loss: 0.257\n",
      "epoch: 4, lr: 0.00100 iter_num: 3800, time: 566.487, loss: 0.362\n",
      "epoch: 4, lr: 0.00100 iter_num: 3900, time: 568.963, loss: 0.204\n",
      "epoch: 4, lr: 0.00100 iter_num: 4000, time: 571.239, loss: 0.401\n",
      "epoch: 4, lr: 0.00100 iter_num: 4100, time: 573.542, loss: 0.170\n",
      "epoch: 4, lr: 0.00100 iter_num: 4200, time: 576.131, loss: 0.214\n",
      "epoch: 4, lr: 0.00100 iter_num: 4300, time: 578.513, loss: 0.272\n",
      "epoch: 4, lr: 0.00100 iter_num: 4400, time: 580.832, loss: 0.179\n",
      "epoch: 4, lr: 0.00100 iter_num: 4500, time: 583.129, loss: 0.216\n",
      "epoch: 4, lr: 0.00100 iter_num: 4600, time: 585.327, loss: 0.234\n",
      "epoch: 4, lr: 0.00100 iter_num: 4700, time: 587.588, loss: 0.163\n",
      "epoch: 4, lr: 0.00100 iter_num: 4800, time: 589.938, loss: 0.226\n",
      "epoch: 4, lr: 0.00100 iter_num: 4900, time: 592.151, loss: 0.258\n",
      "Epoch 5, Test Accuracy: 91.34%\n",
      "Model saved with accuracy: 91.34%\n",
      "epoch: 5, lr: 0.00100 iter_num: 0, time: 622.131, loss: 0.234\n",
      "epoch: 5, lr: 0.00100 iter_num: 100, time: 624.495, loss: 0.210\n",
      "epoch: 5, lr: 0.00100 iter_num: 200, time: 626.852, loss: 0.402\n",
      "epoch: 5, lr: 0.00100 iter_num: 300, time: 629.321, loss: 0.214\n",
      "epoch: 5, lr: 0.00100 iter_num: 400, time: 631.496, loss: 0.201\n",
      "epoch: 5, lr: 0.00100 iter_num: 500, time: 633.607, loss: 0.208\n",
      "epoch: 5, lr: 0.00100 iter_num: 600, time: 635.985, loss: 0.186\n",
      "epoch: 5, lr: 0.00100 iter_num: 700, time: 638.236, loss: 0.271\n",
      "epoch: 5, lr: 0.00100 iter_num: 800, time: 640.596, loss: 0.169\n",
      "epoch: 5, lr: 0.00100 iter_num: 900, time: 642.883, loss: 0.279\n",
      "epoch: 5, lr: 0.00100 iter_num: 1000, time: 645.157, loss: 0.265\n",
      "epoch: 5, lr: 0.00100 iter_num: 1100, time: 647.432, loss: 0.241\n",
      "epoch: 5, lr: 0.00100 iter_num: 1200, time: 649.860, loss: 0.307\n",
      "epoch: 5, lr: 0.00100 iter_num: 1300, time: 652.151, loss: 0.242\n",
      "epoch: 5, lr: 0.00100 iter_num: 1400, time: 654.521, loss: 0.260\n",
      "epoch: 5, lr: 0.00100 iter_num: 1500, time: 657.045, loss: 0.231\n",
      "epoch: 5, lr: 0.00100 iter_num: 1600, time: 659.353, loss: 0.273\n",
      "epoch: 5, lr: 0.00100 iter_num: 1700, time: 661.761, loss: 0.146\n",
      "epoch: 5, lr: 0.00100 iter_num: 1800, time: 664.002, loss: 0.277\n",
      "epoch: 5, lr: 0.00100 iter_num: 1900, time: 666.467, loss: 0.284\n",
      "epoch: 5, lr: 0.00100 iter_num: 2000, time: 668.750, loss: 0.205\n",
      "epoch: 5, lr: 0.00100 iter_num: 2100, time: 671.022, loss: 0.231\n",
      "epoch: 5, lr: 0.00100 iter_num: 2200, time: 673.315, loss: 0.241\n",
      "epoch: 5, lr: 0.00100 iter_num: 2300, time: 675.592, loss: 0.221\n",
      "epoch: 5, lr: 0.00100 iter_num: 2400, time: 677.953, loss: 0.219\n",
      "epoch: 5, lr: 0.00100 iter_num: 2500, time: 680.187, loss: 0.186\n",
      "epoch: 5, lr: 0.00100 iter_num: 2600, time: 682.642, loss: 0.325\n",
      "epoch: 5, lr: 0.00100 iter_num: 2700, time: 685.130, loss: 0.224\n",
      "epoch: 5, lr: 0.00100 iter_num: 2800, time: 687.398, loss: 0.198\n",
      "epoch: 5, lr: 0.00100 iter_num: 2900, time: 689.678, loss: 0.220\n",
      "epoch: 5, lr: 0.00100 iter_num: 3000, time: 691.928, loss: 0.203\n",
      "epoch: 5, lr: 0.00100 iter_num: 3100, time: 694.371, loss: 0.251\n",
      "epoch: 5, lr: 0.00100 iter_num: 3200, time: 696.655, loss: 0.153\n",
      "epoch: 5, lr: 0.00100 iter_num: 3300, time: 698.903, loss: 0.219\n",
      "epoch: 5, lr: 0.00100 iter_num: 3400, time: 701.222, loss: 0.161\n",
      "epoch: 5, lr: 0.00100 iter_num: 3500, time: 703.519, loss: 0.301\n",
      "epoch: 5, lr: 0.00100 iter_num: 3600, time: 706.059, loss: 0.245\n",
      "epoch: 5, lr: 0.00100 iter_num: 3700, time: 708.481, loss: 0.215\n",
      "epoch: 5, lr: 0.00100 iter_num: 3800, time: 710.967, loss: 0.251\n",
      "epoch: 5, lr: 0.00100 iter_num: 3900, time: 713.392, loss: 0.255\n",
      "epoch: 5, lr: 0.00100 iter_num: 4000, time: 715.735, loss: 0.220\n",
      "epoch: 5, lr: 0.00100 iter_num: 4100, time: 718.062, loss: 0.225\n",
      "epoch: 5, lr: 0.00100 iter_num: 4200, time: 720.421, loss: 0.226\n",
      "epoch: 5, lr: 0.00100 iter_num: 4300, time: 722.818, loss: 0.187\n",
      "epoch: 5, lr: 0.00100 iter_num: 4400, time: 724.937, loss: 0.186\n",
      "epoch: 5, lr: 0.00100 iter_num: 4500, time: 727.250, loss: 0.176\n",
      "epoch: 5, lr: 0.00100 iter_num: 4600, time: 729.545, loss: 0.248\n",
      "epoch: 5, lr: 0.00100 iter_num: 4700, time: 732.026, loss: 0.187\n",
      "epoch: 5, lr: 0.00100 iter_num: 4800, time: 734.503, loss: 0.201\n",
      "epoch: 5, lr: 0.00100 iter_num: 4900, time: 736.880, loss: 0.227\n",
      "epoch: 6, lr: 0.00100 iter_num: 0, time: 739.289, loss: 0.299\n",
      "epoch: 6, lr: 0.00100 iter_num: 100, time: 741.601, loss: 0.160\n",
      "epoch: 6, lr: 0.00100 iter_num: 200, time: 743.856, loss: 0.291\n",
      "epoch: 6, lr: 0.00100 iter_num: 300, time: 746.390, loss: 0.295\n",
      "epoch: 6, lr: 0.00100 iter_num: 400, time: 748.695, loss: 0.198\n",
      "epoch: 6, lr: 0.00100 iter_num: 500, time: 750.963, loss: 0.241\n",
      "epoch: 6, lr: 0.00100 iter_num: 600, time: 753.216, loss: 0.221\n",
      "epoch: 6, lr: 0.00100 iter_num: 700, time: 755.422, loss: 0.206\n",
      "epoch: 6, lr: 0.00100 iter_num: 800, time: 757.846, loss: 0.197\n",
      "epoch: 6, lr: 0.00100 iter_num: 900, time: 760.394, loss: 0.216\n",
      "epoch: 6, lr: 0.00100 iter_num: 1000, time: 762.817, loss: 0.179\n",
      "epoch: 6, lr: 0.00100 iter_num: 1100, time: 765.073, loss: 0.216\n",
      "epoch: 6, lr: 0.00100 iter_num: 1200, time: 767.381, loss: 0.327\n",
      "epoch: 6, lr: 0.00100 iter_num: 1300, time: 769.668, loss: 0.215\n",
      "epoch: 6, lr: 0.00100 iter_num: 1400, time: 772.082, loss: 0.217\n",
      "epoch: 6, lr: 0.00100 iter_num: 1500, time: 774.387, loss: 0.206\n",
      "epoch: 6, lr: 0.00100 iter_num: 1600, time: 776.638, loss: 0.264\n",
      "epoch: 6, lr: 0.00100 iter_num: 1700, time: 778.870, loss: 0.244\n",
      "epoch: 6, lr: 0.00100 iter_num: 1800, time: 781.122, loss: 0.219\n",
      "epoch: 6, lr: 0.00100 iter_num: 1900, time: 783.593, loss: 0.180\n",
      "epoch: 6, lr: 0.00100 iter_num: 2000, time: 786.037, loss: 0.265\n",
      "epoch: 6, lr: 0.00100 iter_num: 2100, time: 788.395, loss: 0.277\n",
      "epoch: 6, lr: 0.00100 iter_num: 2200, time: 790.723, loss: 0.140\n",
      "epoch: 6, lr: 0.00100 iter_num: 2300, time: 793.010, loss: 0.335\n",
      "epoch: 6, lr: 0.00100 iter_num: 2400, time: 795.267, loss: 0.218\n",
      "epoch: 6, lr: 0.00100 iter_num: 2500, time: 797.630, loss: 0.290\n",
      "epoch: 6, lr: 0.00100 iter_num: 2600, time: 799.987, loss: 0.241\n",
      "epoch: 6, lr: 0.00100 iter_num: 2700, time: 802.241, loss: 0.229\n",
      "epoch: 6, lr: 0.00100 iter_num: 2800, time: 804.460, loss: 0.223\n",
      "epoch: 6, lr: 0.00100 iter_num: 2900, time: 806.597, loss: 0.250\n",
      "epoch: 6, lr: 0.00100 iter_num: 3000, time: 809.063, loss: 0.257\n",
      "epoch: 6, lr: 0.00100 iter_num: 3100, time: 811.579, loss: 0.316\n",
      "epoch: 6, lr: 0.00100 iter_num: 3200, time: 814.045, loss: 0.244\n",
      "epoch: 6, lr: 0.00100 iter_num: 3300, time: 816.458, loss: 0.273\n",
      "epoch: 6, lr: 0.00100 iter_num: 3400, time: 818.765, loss: 0.188\n",
      "epoch: 6, lr: 0.00100 iter_num: 3500, time: 821.025, loss: 0.192\n",
      "epoch: 6, lr: 0.00100 iter_num: 3600, time: 823.482, loss: 0.292\n",
      "epoch: 6, lr: 0.00100 iter_num: 3700, time: 825.800, loss: 0.155\n",
      "epoch: 6, lr: 0.00100 iter_num: 3800, time: 828.117, loss: 0.217\n",
      "epoch: 6, lr: 0.00100 iter_num: 3900, time: 830.450, loss: 0.217\n",
      "epoch: 6, lr: 0.00100 iter_num: 4000, time: 832.747, loss: 0.246\n",
      "epoch: 6, lr: 0.00100 iter_num: 4100, time: 835.323, loss: 0.202\n",
      "epoch: 6, lr: 0.00100 iter_num: 4200, time: 837.495, loss: 0.267\n",
      "epoch: 6, lr: 0.00100 iter_num: 4300, time: 839.823, loss: 0.268\n",
      "epoch: 6, lr: 0.00100 iter_num: 4400, time: 842.162, loss: 0.263\n",
      "epoch: 6, lr: 0.00100 iter_num: 4500, time: 844.437, loss: 0.242\n",
      "epoch: 6, lr: 0.00100 iter_num: 4600, time: 846.842, loss: 0.247\n",
      "epoch: 6, lr: 0.00100 iter_num: 4700, time: 849.297, loss: 0.315\n",
      "epoch: 6, lr: 0.00100 iter_num: 4800, time: 851.571, loss: 0.257\n",
      "epoch: 6, lr: 0.00100 iter_num: 4900, time: 853.853, loss: 0.298\n",
      "epoch: 7, lr: 0.00100 iter_num: 0, time: 856.116, loss: 0.206\n",
      "epoch: 7, lr: 0.00100 iter_num: 100, time: 858.766, loss: 0.328\n",
      "epoch: 7, lr: 0.00100 iter_num: 200, time: 861.101, loss: 0.202\n",
      "epoch: 7, lr: 0.00100 iter_num: 300, time: 863.400, loss: 0.164\n",
      "epoch: 7, lr: 0.00100 iter_num: 400, time: 865.767, loss: 0.162\n",
      "epoch: 7, lr: 0.00100 iter_num: 500, time: 867.973, loss: 0.257\n",
      "epoch: 7, lr: 0.00100 iter_num: 600, time: 870.238, loss: 0.255\n",
      "epoch: 7, lr: 0.00100 iter_num: 700, time: 872.755, loss: 0.218\n",
      "epoch: 7, lr: 0.00100 iter_num: 800, time: 875.055, loss: 0.293\n",
      "epoch: 7, lr: 0.00100 iter_num: 900, time: 877.383, loss: 0.213\n",
      "epoch: 7, lr: 0.00100 iter_num: 1000, time: 879.642, loss: 0.176\n",
      "epoch: 7, lr: 0.00100 iter_num: 1100, time: 882.002, loss: 0.249\n",
      "epoch: 7, lr: 0.00100 iter_num: 1200, time: 884.504, loss: 0.203\n",
      "epoch: 7, lr: 0.00100 iter_num: 1300, time: 886.892, loss: 0.204\n",
      "epoch: 7, lr: 0.00100 iter_num: 1400, time: 889.226, loss: 0.216\n",
      "epoch: 7, lr: 0.00100 iter_num: 1500, time: 891.501, loss: 0.255\n",
      "epoch: 7, lr: 0.00100 iter_num: 1600, time: 893.750, loss: 0.190\n",
      "epoch: 7, lr: 0.00100 iter_num: 1700, time: 896.253, loss: 0.267\n",
      "epoch: 7, lr: 0.00100 iter_num: 1800, time: 898.585, loss: 0.191\n",
      "epoch: 7, lr: 0.00100 iter_num: 1900, time: 900.847, loss: 0.153\n",
      "epoch: 7, lr: 0.00100 iter_num: 2000, time: 903.104, loss: 0.216\n",
      "epoch: 7, lr: 0.00100 iter_num: 2100, time: 905.438, loss: 0.312\n",
      "epoch: 7, lr: 0.00100 iter_num: 2200, time: 907.903, loss: 0.238\n",
      "epoch: 7, lr: 0.00100 iter_num: 2300, time: 910.423, loss: 0.252\n",
      "epoch: 7, lr: 0.00100 iter_num: 2400, time: 912.771, loss: 0.180\n",
      "epoch: 7, lr: 0.00100 iter_num: 2500, time: 915.098, loss: 0.277\n",
      "epoch: 7, lr: 0.00100 iter_num: 2600, time: 917.360, loss: 0.212\n",
      "epoch: 7, lr: 0.00100 iter_num: 2700, time: 919.804, loss: 0.251\n",
      "epoch: 7, lr: 0.00100 iter_num: 2800, time: 922.210, loss: 0.288\n",
      "epoch: 7, lr: 0.00100 iter_num: 2900, time: 924.509, loss: 0.308\n",
      "epoch: 7, lr: 0.00100 iter_num: 3000, time: 926.801, loss: 0.222\n",
      "epoch: 7, lr: 0.00100 iter_num: 3100, time: 929.147, loss: 0.277\n",
      "epoch: 7, lr: 0.00100 iter_num: 3200, time: 931.731, loss: 0.253\n",
      "epoch: 7, lr: 0.00100 iter_num: 3300, time: 934.120, loss: 0.216\n",
      "epoch: 7, lr: 0.00100 iter_num: 3400, time: 936.391, loss: 0.205\n",
      "epoch: 7, lr: 0.00100 iter_num: 3500, time: 938.674, loss: 0.168\n",
      "epoch: 7, lr: 0.00100 iter_num: 3600, time: 940.984, loss: 0.248\n",
      "epoch: 7, lr: 0.00100 iter_num: 3700, time: 943.323, loss: 0.230\n",
      "epoch: 7, lr: 0.00100 iter_num: 3800, time: 945.727, loss: 0.144\n",
      "epoch: 7, lr: 0.00100 iter_num: 3900, time: 948.112, loss: 0.331\n",
      "epoch: 7, lr: 0.00100 iter_num: 4000, time: 950.460, loss: 0.254\n",
      "epoch: 7, lr: 0.00100 iter_num: 4100, time: 952.967, loss: 0.224\n",
      "epoch: 7, lr: 0.00100 iter_num: 4200, time: 955.463, loss: 0.227\n",
      "epoch: 7, lr: 0.00100 iter_num: 4300, time: 957.755, loss: 0.180\n",
      "epoch: 7, lr: 0.00100 iter_num: 4400, time: 960.179, loss: 0.371\n",
      "epoch: 7, lr: 0.00100 iter_num: 4500, time: 962.498, loss: 0.239\n",
      "epoch: 7, lr: 0.00100 iter_num: 4600, time: 964.758, loss: 0.235\n",
      "epoch: 7, lr: 0.00100 iter_num: 4700, time: 967.215, loss: 0.183\n",
      "epoch: 7, lr: 0.00100 iter_num: 4800, time: 969.333, loss: 0.176\n",
      "epoch: 7, lr: 0.00100 iter_num: 4900, time: 971.661, loss: 0.194\n",
      "epoch: 8, lr: 0.00100 iter_num: 0, time: 974.093, loss: 0.247\n",
      "epoch: 8, lr: 0.00100 iter_num: 100, time: 976.470, loss: 0.215\n",
      "epoch: 8, lr: 0.00100 iter_num: 200, time: 978.898, loss: 0.291\n",
      "epoch: 8, lr: 0.00100 iter_num: 300, time: 981.387, loss: 0.272\n",
      "epoch: 8, lr: 0.00100 iter_num: 400, time: 983.654, loss: 0.241\n",
      "epoch: 8, lr: 0.00100 iter_num: 500, time: 985.919, loss: 0.199\n",
      "epoch: 8, lr: 0.00100 iter_num: 600, time: 988.441, loss: 0.191\n",
      "epoch: 8, lr: 0.00100 iter_num: 700, time: 990.767, loss: 0.256\n",
      "epoch: 8, lr: 0.00100 iter_num: 800, time: 993.100, loss: 0.224\n",
      "epoch: 8, lr: 0.00100 iter_num: 900, time: 995.438, loss: 0.251\n",
      "epoch: 8, lr: 0.00100 iter_num: 1000, time: 997.742, loss: 0.247\n",
      "epoch: 8, lr: 0.00100 iter_num: 1100, time: 1000.263, loss: 0.213\n",
      "epoch: 8, lr: 0.00100 iter_num: 1200, time: 1002.644, loss: 0.235\n",
      "epoch: 8, lr: 0.00100 iter_num: 1300, time: 1004.912, loss: 0.255\n",
      "epoch: 8, lr: 0.00100 iter_num: 1400, time: 1007.187, loss: 0.267\n",
      "epoch: 8, lr: 0.00100 iter_num: 1500, time: 1009.675, loss: 0.211\n",
      "epoch: 8, lr: 0.00100 iter_num: 1600, time: 1011.983, loss: 0.208\n",
      "epoch: 8, lr: 0.00100 iter_num: 1700, time: 1014.283, loss: 0.186\n",
      "epoch: 8, lr: 0.00100 iter_num: 1800, time: 1016.543, loss: 0.199\n",
      "epoch: 8, lr: 0.00100 iter_num: 1900, time: 1019.064, loss: 0.141\n",
      "epoch: 8, lr: 0.00100 iter_num: 2000, time: 1021.491, loss: 0.259\n",
      "epoch: 8, lr: 0.00100 iter_num: 2100, time: 1023.991, loss: 0.190\n",
      "epoch: 8, lr: 0.00100 iter_num: 2200, time: 1026.209, loss: 0.250\n",
      "epoch: 8, lr: 0.00100 iter_num: 2300, time: 1028.508, loss: 0.251\n",
      "epoch: 8, lr: 0.00100 iter_num: 2400, time: 1030.789, loss: 0.264\n",
      "epoch: 8, lr: 0.00100 iter_num: 2500, time: 1033.195, loss: 0.227\n",
      "epoch: 8, lr: 0.00100 iter_num: 2600, time: 1035.492, loss: 0.168\n",
      "epoch: 8, lr: 0.00100 iter_num: 2700, time: 1037.765, loss: 0.242\n",
      "epoch: 8, lr: 0.00100 iter_num: 2800, time: 1040.282, loss: 0.217\n",
      "epoch: 8, lr: 0.00100 iter_num: 2900, time: 1042.681, loss: 0.272\n",
      "epoch: 8, lr: 0.00100 iter_num: 3000, time: 1045.107, loss: 0.182\n",
      "epoch: 8, lr: 0.00100 iter_num: 3100, time: 1047.558, loss: 0.220\n",
      "epoch: 8, lr: 0.00100 iter_num: 3200, time: 1049.809, loss: 0.184\n",
      "epoch: 8, lr: 0.00100 iter_num: 3300, time: 1052.161, loss: 0.284\n",
      "epoch: 8, lr: 0.00100 iter_num: 3400, time: 1054.621, loss: 0.278\n",
      "epoch: 8, lr: 0.00100 iter_num: 3500, time: 1056.902, loss: 0.215\n",
      "epoch: 8, lr: 0.00100 iter_num: 3600, time: 1059.226, loss: 0.235\n",
      "epoch: 8, lr: 0.00100 iter_num: 3700, time: 1061.705, loss: 0.239\n",
      "epoch: 8, lr: 0.00100 iter_num: 3800, time: 1063.888, loss: 0.186\n",
      "epoch: 8, lr: 0.00100 iter_num: 3900, time: 1066.276, loss: 0.376\n",
      "epoch: 8, lr: 0.00100 iter_num: 4000, time: 1068.707, loss: 0.211\n",
      "epoch: 8, lr: 0.00100 iter_num: 4100, time: 1071.132, loss: 0.282\n",
      "epoch: 8, lr: 0.00100 iter_num: 4200, time: 1073.426, loss: 0.429\n",
      "epoch: 8, lr: 0.00100 iter_num: 4300, time: 1075.892, loss: 0.245\n",
      "epoch: 8, lr: 0.00100 iter_num: 4400, time: 1078.163, loss: 0.207\n",
      "epoch: 8, lr: 0.00100 iter_num: 4500, time: 1080.585, loss: 0.216\n",
      "epoch: 8, lr: 0.00100 iter_num: 4600, time: 1082.835, loss: 0.227\n",
      "epoch: 8, lr: 0.00100 iter_num: 4700, time: 1085.155, loss: 0.284\n",
      "epoch: 8, lr: 0.00100 iter_num: 4800, time: 1087.637, loss: 0.246\n",
      "epoch: 8, lr: 0.00100 iter_num: 4900, time: 1090.141, loss: 0.191\n",
      "epoch: 9, lr: 0.00100 iter_num: 0, time: 1092.424, loss: 0.208\n",
      "epoch: 9, lr: 0.00100 iter_num: 100, time: 1094.629, loss: 0.199\n",
      "epoch: 9, lr: 0.00100 iter_num: 200, time: 1097.092, loss: 0.213\n",
      "epoch: 9, lr: 0.00100 iter_num: 300, time: 1099.253, loss: 0.235\n",
      "epoch: 9, lr: 0.00100 iter_num: 400, time: 1101.640, loss: 0.255\n",
      "epoch: 9, lr: 0.00100 iter_num: 500, time: 1103.932, loss: 0.281\n",
      "epoch: 9, lr: 0.00100 iter_num: 600, time: 1106.194, loss: 0.238\n",
      "epoch: 9, lr: 0.00100 iter_num: 700, time: 1108.776, loss: 0.225\n",
      "epoch: 9, lr: 0.00100 iter_num: 800, time: 1111.116, loss: 0.177\n",
      "epoch: 9, lr: 0.00100 iter_num: 900, time: 1113.264, loss: 0.212\n",
      "epoch: 9, lr: 0.00100 iter_num: 1000, time: 1115.674, loss: 0.218\n",
      "epoch: 9, lr: 0.00100 iter_num: 1100, time: 1118.107, loss: 0.341\n",
      "epoch: 9, lr: 0.00100 iter_num: 1200, time: 1120.489, loss: 0.247\n",
      "epoch: 9, lr: 0.00100 iter_num: 1300, time: 1122.923, loss: 0.198\n",
      "epoch: 9, lr: 0.00100 iter_num: 1400, time: 1125.174, loss: 0.195\n",
      "epoch: 9, lr: 0.00100 iter_num: 1500, time: 1127.500, loss: 0.241\n",
      "epoch: 9, lr: 0.00100 iter_num: 1600, time: 1130.136, loss: 0.245\n",
      "epoch: 9, lr: 0.00100 iter_num: 1700, time: 1132.463, loss: 0.172\n",
      "epoch: 9, lr: 0.00100 iter_num: 1800, time: 1134.757, loss: 0.213\n",
      "epoch: 9, lr: 0.00100 iter_num: 1900, time: 1137.165, loss: 0.190\n",
      "epoch: 9, lr: 0.00100 iter_num: 2000, time: 1139.425, loss: 0.180\n",
      "epoch: 9, lr: 0.00100 iter_num: 2100, time: 1141.861, loss: 0.252\n",
      "epoch: 9, lr: 0.00100 iter_num: 2200, time: 1144.249, loss: 0.276\n",
      "epoch: 9, lr: 0.00100 iter_num: 2300, time: 1146.451, loss: 0.194\n",
      "epoch: 9, lr: 0.00100 iter_num: 2400, time: 1148.972, loss: 0.195\n",
      "epoch: 9, lr: 0.00100 iter_num: 2500, time: 1151.465, loss: 0.246\n",
      "epoch: 9, lr: 0.00100 iter_num: 2600, time: 1153.723, loss: 0.198\n",
      "epoch: 9, lr: 0.00100 iter_num: 2700, time: 1156.088, loss: 0.193\n",
      "epoch: 9, lr: 0.00100 iter_num: 2800, time: 1158.475, loss: 0.189\n",
      "epoch: 9, lr: 0.00100 iter_num: 2900, time: 1160.785, loss: 0.259\n",
      "epoch: 9, lr: 0.00100 iter_num: 3000, time: 1163.281, loss: 0.294\n",
      "epoch: 9, lr: 0.00100 iter_num: 3100, time: 1165.415, loss: 0.223\n",
      "epoch: 9, lr: 0.00100 iter_num: 3200, time: 1167.613, loss: 0.139\n",
      "epoch: 9, lr: 0.00100 iter_num: 3300, time: 1170.222, loss: 0.181\n",
      "epoch: 9, lr: 0.00100 iter_num: 3400, time: 1172.579, loss: 0.319\n",
      "epoch: 9, lr: 0.00100 iter_num: 3500, time: 1174.886, loss: 0.235\n",
      "epoch: 9, lr: 0.00100 iter_num: 3600, time: 1177.332, loss: 0.181\n",
      "epoch: 9, lr: 0.00100 iter_num: 3700, time: 1179.609, loss: 0.180\n",
      "epoch: 9, lr: 0.00100 iter_num: 3800, time: 1181.999, loss: 0.237\n",
      "epoch: 9, lr: 0.00100 iter_num: 3900, time: 1184.357, loss: 0.209\n",
      "epoch: 9, lr: 0.00100 iter_num: 4000, time: 1186.636, loss: 0.136\n",
      "epoch: 9, lr: 0.00100 iter_num: 4100, time: 1188.868, loss: 0.231\n",
      "epoch: 9, lr: 0.00100 iter_num: 4200, time: 1191.432, loss: 0.219\n",
      "epoch: 9, lr: 0.00100 iter_num: 4300, time: 1193.591, loss: 0.223\n",
      "epoch: 9, lr: 0.00100 iter_num: 4400, time: 1195.960, loss: 0.227\n",
      "epoch: 9, lr: 0.00100 iter_num: 4500, time: 1198.336, loss: 0.213\n",
      "epoch: 9, lr: 0.00100 iter_num: 4600, time: 1200.642, loss: 0.221\n",
      "epoch: 9, lr: 0.00100 iter_num: 4700, time: 1203.064, loss: 0.158\n",
      "epoch: 9, lr: 0.00100 iter_num: 4800, time: 1205.383, loss: 0.199\n",
      "epoch: 9, lr: 0.00100 iter_num: 4900, time: 1207.673, loss: 0.186\n",
      "Epoch 10, Test Accuracy: 92.37%\n",
      "Model saved with accuracy: 92.37%\n",
      "epoch: 10, lr: 0.00100 iter_num: 0, time: 1235.720, loss: 0.203\n",
      "epoch: 10, lr: 0.00100 iter_num: 100, time: 1238.053, loss: 0.248\n",
      "epoch: 10, lr: 0.00100 iter_num: 200, time: 1240.287, loss: 0.214\n",
      "epoch: 10, lr: 0.00100 iter_num: 300, time: 1242.554, loss: 0.241\n",
      "epoch: 10, lr: 0.00100 iter_num: 400, time: 1244.776, loss: 0.181\n",
      "epoch: 10, lr: 0.00100 iter_num: 500, time: 1247.080, loss: 0.221\n",
      "epoch: 10, lr: 0.00100 iter_num: 600, time: 1249.719, loss: 0.210\n",
      "epoch: 10, lr: 0.00100 iter_num: 700, time: 1252.059, loss: 0.214\n",
      "epoch: 10, lr: 0.00100 iter_num: 800, time: 1254.534, loss: 0.180\n",
      "epoch: 10, lr: 0.00100 iter_num: 900, time: 1256.864, loss: 0.184\n",
      "epoch: 10, lr: 0.00100 iter_num: 1000, time: 1259.255, loss: 0.323\n",
      "epoch: 10, lr: 0.00100 iter_num: 1100, time: 1261.585, loss: 0.181\n",
      "epoch: 10, lr: 0.00100 iter_num: 1200, time: 1263.692, loss: 0.248\n",
      "epoch: 10, lr: 0.00100 iter_num: 1300, time: 1265.985, loss: 0.210\n",
      "epoch: 10, lr: 0.00100 iter_num: 1400, time: 1268.590, loss: 0.177\n",
      "epoch: 10, lr: 0.00100 iter_num: 1500, time: 1270.998, loss: 0.209\n",
      "epoch: 10, lr: 0.00100 iter_num: 1600, time: 1273.261, loss: 0.182\n",
      "epoch: 10, lr: 0.00100 iter_num: 1700, time: 1275.662, loss: 0.226\n",
      "epoch: 10, lr: 0.00100 iter_num: 1800, time: 1277.945, loss: 0.178\n",
      "epoch: 10, lr: 0.00100 iter_num: 1900, time: 1280.196, loss: 0.150\n",
      "epoch: 10, lr: 0.00100 iter_num: 2000, time: 1282.450, loss: 0.218\n",
      "epoch: 10, lr: 0.00100 iter_num: 2100, time: 1284.763, loss: 0.237\n",
      "epoch: 10, lr: 0.00100 iter_num: 2200, time: 1287.070, loss: 0.186\n",
      "epoch: 10, lr: 0.00100 iter_num: 2300, time: 1289.601, loss: 0.208\n",
      "epoch: 10, lr: 0.00100 iter_num: 2400, time: 1291.870, loss: 0.195\n",
      "epoch: 10, lr: 0.00100 iter_num: 2500, time: 1294.231, loss: 0.253\n",
      "epoch: 10, lr: 0.00100 iter_num: 2600, time: 1296.568, loss: 0.203\n",
      "epoch: 10, lr: 0.00100 iter_num: 2700, time: 1299.111, loss: 0.196\n",
      "epoch: 10, lr: 0.00100 iter_num: 2800, time: 1301.413, loss: 0.219\n",
      "epoch: 10, lr: 0.00100 iter_num: 2900, time: 1303.707, loss: 0.304\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from cac import test_accuracy\n",
    "from condition import add_label_noise\n",
    "\n",
    "noise_rate=0.1\n",
    "# 初始化\n",
    "start_time = time.time()\n",
    "total_loss = []\n",
    "accuracy_list = []  # 存储每次epoch的准确率\n",
    "best_accuracy = 0.0  # 存储最高准确率\n",
    "\n",
    "for epoch in range(epochs):  # 循环遍历数据集多次\n",
    "    scheduler.step()\n",
    "    if epoch % 5 == 0:\n",
    "        accuracy = test_accuracy(model, testloader, label_hash_codes,device)\n",
    "        print(f'Epoch {epoch}, Test Accuracy: {accuracy}%')\n",
    "        accuracy_list.append(accuracy)  # 添加当前准确率到列表\n",
    "        \n",
    "        # 检查是否是最高准确率\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            # 保存模型\n",
    "            torch.save(model.state_dict(), './model/nr_'+str(noise_rate)+'_model.pth')\n",
    "            print(\"Model saved with accuracy: {:.2f}%\".format(best_accuracy))\n",
    "    \n",
    "    for iter, traindata in enumerate(trainloader, 0):\n",
    "        inputs, labels = traindata\n",
    "        labels = add_label_noise(labels.numpy(), noise_rate=noise_rate, num_classes=10)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        inputs = model(inputs)\n",
    "        cat_codes = label_hash_codes[labels].to(device) \n",
    "        criterion = nn.BCELoss().to(device)\n",
    "        center_loss = criterion(0.5*(inputs+1),0.5*(cat_codes+1))\n",
    "\n",
    "        Q_loss = torch.mean((torch.abs(inputs)-1.0)**2)\n",
    "        loss = center_loss + lambda1*Q_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.data.to(device).numpy)\n",
    "        if iter % 500 == 0:\n",
    "            end_time1 = time.time()\n",
    "            print('epoch: %d, lr: %.5f iter_num: %d, time: %.3f, loss: %.3f' % \n",
    "                  (epoch, lr, iter, (end_time1-start_time), loss))\n",
    "        \n",
    "end_time = time.time()\n",
    "epoch_loss = np.mean(total_loss)\n",
    "print('Finished Training')\n",
    "\n",
    "# 打印所有epoch的准确率\n",
    "print(\"Epoch accuracies:\", accuracy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen -S exp "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
